{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqo3TMlUtYT0"
      },
      "source": [
        "#Summer 2025 project  -   StoryGenerator: Creative text generation using a Recurrent Neural Network(RNN)\n",
        "\n",
        "\n",
        "By Ashar Ali\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #RUN THIS BOX ONCE (SO THERE IS  NO \"TORCH TEXT  MODULE NOT FOUND\" ERROR IN PART 1 -IT INSTALLS COMPATBLE VERSIONS )\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchtext==0.15.2 --index-url https://download.pytorch.org/whl/cpu\n"
      ],
      "metadata": {
        "id": "AUwce9DyudzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 0 - IMPORTING REQUIRED LIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
        "\n",
        "import numpy  # For numericals\n",
        "import re # For regular expressions use\n",
        "import matplotlib.pyplot as plt # For visiualsations\n",
        "\n",
        "import torchtext\n",
        "\"\"\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n"
      ],
      "metadata": {
        "id": "cD6LJZUVHIz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDYx8GReC4E"
      },
      "outputs": [],
      "source": [
        "# PART 1 - LOADING AND READING THE DATASET\n",
        "\n",
        "# Download Grimm's Fairy Tales data set (plain text, UTF-8 encoded)\n",
        "!wget https://www.gutenberg.org/files/2591/2591-0.txt -O grimms_fairy_tales.txt\n",
        "\n",
        "#Load the dataset into a list of text lines to read\n",
        "#For simplicity, removes punctaion and uses lowercase\n",
        "def read_tales():\n",
        "  with open(\"grimms_fairy_tales.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "  return [re.sub('[^A-Za-z] + ', ' ',line).strip().lower() for line in lines]\n",
        "\n",
        "# Output number of lines in corpurs and o\n",
        "lines = read_tales()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "\n",
        "#Print first 120 lines\n",
        "for line in lines[0:120]:\n",
        "  print(line)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2 - PREAPERING THE DATA (Tokenisation etc)\n",
        "\n",
        "# Ignore traceback error that may occur\n",
        "\n",
        "# Tokenisation using torch text to break text into smaller peices\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data_lines):\n",
        "    for line in data_lines:\n",
        "        yield tokenizer(line)\n",
        "\n",
        "# Crearting vocab to map tokens to indices\n",
        "vocab = build_vocab_from_iterator(yield_tokens(lines), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "\n",
        "# Create input/target sequences using a sliding window\n",
        "tokenised_lines = [tokenizer(line) for line in lines]\n",
        "encoded_lines = [vocab(tokens) for tokens in tokenised_lines if len(tokens) > 0]\n",
        "sequence_length = 5\n",
        "input_sequences = []\n",
        "target_words = []\n",
        "\n",
        "for line in encoded_lines:\n",
        "    if len(line) <= sequence_length:\n",
        "        continue\n",
        "    for i in range(len(line) - sequence_length):\n",
        "        input_seq = line[i:i+sequence_length]\n",
        "        target = line[i+sequence_length]\n",
        "        input_sequences.append(input_seq)\n",
        "        target_words.append(target)\n",
        "\n",
        "X = torch.tensor(input_sequences, dtype=torch.long)\n",
        "y = torch.tensor(target_words, dtype=torch.long)\n",
        "\n",
        "print(f\"Input shape: {X.shape}, Target shape: {y.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xxvtoUdyu5It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qysFtZ5GtWSy"
      },
      "outputs": [],
      "source": [
        "# PART 3 - MODEL DEFINITION\n",
        "\n",
        "\n",
        "class StoryLSTM(nn.Module):\n",
        "  \"\"\"The RNN model to generete creative text\"\"\"\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):\n",
        "    super(StoryLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    out, _ = self.lstm(x)\n",
        "    out = out[:, -1, :]\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 4 - MODEL TRAINING (will take some time -  use t4 GPU google collab )\n",
        "\n",
        "#Hyperparameters and model initilisation\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "\n",
        "num_epochs = 25\n",
        "batch_size = 64\n",
        "\n",
        "model = StoryLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "\n",
        "\n",
        "# Create dataset and split into train and validation (e.g., 80/20 split)\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Initilise dataloders,loss and optimiser\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Early stopping and running totals\n",
        "best_val_loss = float('inf')\n",
        "patience = 3  # You can tweak this\n",
        "counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    train_loss = running_loss / train_size\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "    val_loss = val_running_loss / val_size\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nxPPdvyWHm3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PART 5 - EVALUATING MODEL PERFORMANCE\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))  # Load early stopped model\n",
        "model.eval()\n",
        "\n",
        "# Hardcoded prompt to give model (CHANGE IT HERE FOR DIFFERENT OUTPUTS)\n",
        "prompt = \"The princess \"\n",
        "\n",
        "temperature = 1.0  #  Controls randomness: lower = more predictable, higher = more creative\n",
        "\n",
        "# Predict next 40 words\n",
        "tokens = tokenizer(prompt.lower())\n",
        "ids = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0)\n",
        "for _ in range(40):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #\n",
        "        output = model(ids[:, -sequence_length:])\n",
        "        logits = output / temperature\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_id = torch.multinomial(probs, num_samples=1)[-1].item()  # ðŸŽ² Sample instead of argmax\n",
        "        ids = torch.cat([ids, torch.tensor([[predicted_id]])], dim=1)\n",
        "\n",
        "generated = [vocab.lookup_token(tok.item()) for tok in ids[0]]\n",
        "print(\"Generated:\\n\", \" \".join(generated))\n"
      ],
      "metadata": {
        "id": "fnDmSsXkHnOj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}